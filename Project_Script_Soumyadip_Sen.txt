
==>To Start the Hive in linux clone:
----------------------------------

->cd $HADOOP_HOME
->sbin/start-all.sh

==> Create and Load dataset from local system:
--------------------------------------------------
-> create database sourcedb;
-> use sourcedb;

1)create table customers(cust_id int,cust_f_name String,cust_m_name string,cust_l_name string,cust_ssn int,cust_street int,
cust_city string,cust_state string,cust_country string,cust_zip int,cust_phone bigint,cust_email string)row 
format delimited fields terminated by ',' lines terminated by '\n' tblproperties ("skip.header.line.count"="1");

2)create table item(item_code int,item_name string,item_price float)row format delimited fields terminated by ',' lines terminated 
by '\n' tblproperties ("skip.header.line.count"="1");

3)create table cust_item(tran_id int,tran_date date,cust_id int,item_code int,no_of_item int)row format delimited fields terminated 
by ',' lines terminated by '\n' tblproperties ("skip.header.line.count"="1");

4)load data local inpath '/home/hduser/Sourcelocal/CUSTOMER.csv' into table customers;

5)load data local inpath '/home/hduser/Sourcelocal/ITEM.csv' into table item;

6)load data local inpath '/home/hduser/Sourcelocal/CUST_ITEM.csv' into table cust_item;


==>Create Database Targetdb to store the outputs:
----------------------------------------------------

->create database targetdb;
->use targetdb;

==>Enable the dynamic partition by using the following commands:
--------------------------------------------------------------------

->set hive.exec.dynamic.partition=true;
->set hive.exec.dynamic.partition.mode=nonstrict;
->spark.sql("set hive.exec.dynamic.partition=true");
->spark.sql("set hive.exec.dynamic.partition.mode= nonstrict");


*************FUNCTIONAL REQUIRMENTS********************
--------------------------------------------------------

==>Requirment 1:
-----------------

->create external table cust_purc_year_mon_summ(tran_id int,tran_date date,cust_id int,cust_f_name varchar(40),cust_m_name varchar(40),cust_l_name varchar(40),
item_code int,item_name varchar(40),item_price double,no_of_item int) partitioned by (year int,month string) row format delimited fields
 terminated by ',' lines terminated  by '\n' stored as parquet location '/user/hduser/outputdir/month_part/';

->insert into targetdb.cust_purc_year_mon_summ partition(year,month) select /*+MAPJOIN(t)*/t.tran_id,t.tran_date,c.cust_id,c.cust_f_name,c.cust_m_name,
c.cust_l_name,i.item_code,i.item_name,i.item_price,t.no_of_item,year(tran_date),date_format(tran_date,'MMMM') as month from cust_item t join 
customer c on c.cust_id=t.cust_id join item i on i.item_code=t.item_code;

===============================================================================================================================================================


==>Requirment 2:
-----------------

->create external table item_least_moved(TRANSACTION_ID int,TRANSACTION_DATE date,CUSTOMER_ID int,CUSTOMER_SSN int,CUSTOMER_NAME varchar(120),
ITEM_CODE int,ITEM_PRICE double,NO_OF_ITEM int,ITEM_NAME varchar(40),TOTAL_VALUE double,YEAR int) row format delimited fields 
terminated by ',' lines terminated  by '\n' stored as parquet location '/user/hduser/outputdir/bill_year/';

->s=spark.sql("insert into targetdb.item_least_moved select ci.tran_id,ci.tran_date,ci.cust_id,c.cust_ssn,
upper( concat(c.cust_f_name,' ', c.cust_m_name,' ',c.cust_l_name)) as NAME,i.item_code,i.item_price,ci.no_of_item, upper(i.item_name), 
round(sum(i.item_price * ci.no_of_item),2) as TOTAL_VALUE,year(ci.tran_date) from sourcedb.customers c 
join sourcedb.cust_item ci on c.cust_id = ci.cust_id join sourcedb.item i on i.item_code = ci.item_code 
where year(ci.tran_date) between 2017 and 2020 group by 1,2,3,4,5,6,7,8,9,11").show()

To show output->spark.sql("select * from targetdb.item_least_moved limit 10").show()

====================================================================================================================================================


==>Requirment 3:
-----------------

->df=spark.sql("select c.cust_state, c.cust_city, round(min(i.item_price*ci.no_of_item),2) as MIN_SALES, 
round(max(i.item_price*ci.no_of_item),2) as MAX_SALES, round(avg(i.item_price*ci.no_of_item),2) as AVG_SALES,year(ci.tran_date) 
as YEAR from sourcedb.customers c join sourcedb.cust_item ci on c.cust_id = ci.cust_id join 
sourcedb.item i on ci.item_code = i.item_code where year(ci.tran_date) in  (2017,2018,2019,2020) group by c.cust_state,c.cust_city,year(ci.tran_date)")

->df.repartition(1).write.format("csv").mode("overwrite").option("header","true").save("/home/hduser/filedata/min_max_avg.csv")

=================================================================================================================================================================

==>Requirment 4:
-----------------

->output=spark.sql("select i.item_code, i.item_name, sum(ci.no_of_item) as QTY_SOLD, year(ci.tran_date) as YEAR, quarter(ci.tran_date) as 
QUARTER,DENSE_RANK() OVER (PARTITION BY year(ci.tran_date),quarter(ci.tran_date) ORDER BY sum(ci.no_of_item) desc) as 
RANK from sourcedb.cust_item ci join sourcedb.item i on ci.item_code = i.item_code where year(ci.tran_date) between 2017 and 2020 and 
quarter(ci.tran_date) between 1 and 4 group by 1,2,4,5 order by 3 desc limit 3")

->output.repartition(1).write.format("csv").mode("overwrite").option("header","true").save("/home/hduser/filedata/top_three.csv")


==>Graph Script (Python):
--------------------------

import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv("C:\\Users\\Ritesh\\Desktop\\Graph.csv")
z = df.values.tolist()
#print(x)
#graph = df[["item_name", "QTY_SOLD"]]
print(z[0][2])
y = []
x = []
#i = len(x)
#print(i)
for i in range(len(z)):
    y.append(z[i][2])
    x.append(z[i][1])
print(y)
print(x)
color = ["r", "b", "g"]
plt.barh(x, y,color=color)
plt.title("Rank of item name according to total quantity sold")
for index, value in enumerate(y):
    plt.text(value, index, str(value))



============================================================THE END======================================================================================